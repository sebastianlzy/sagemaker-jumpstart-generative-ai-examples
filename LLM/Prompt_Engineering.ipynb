{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Large Language Model Prompt Engineering\n",
    "\n",
    "***\n",
    "In this lab, we will cover what is prompt engineering, what are some real world use cases, and advanced prompt engineering insights.\n",
    "***\n",
    "\n",
    "1. [Set Up](#1.-Set-Up)\n",
    "2. [N-shots Prompting Concepts](#2.-N-shots-Prompting-Concepts)\n",
    "3. [Chain of Thought (CoT) Prompting](#3.-Chain-of-Thought-(CoT)-Prompting)\n",
    "4. [Tree of Thoughts (ToT) Prompting](#4.-Tree-of-Thoughts-(ToT)-Prompting)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 1. Set Up\n",
    "\n",
    "The endpoint URL is pointing to a AI21 Jurassic 2 Grande model deployed on Amazon Sagemaker endpoint. This will be the model used for this lab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-10T05:48:10.580488Z",
     "start_time": "2023-07-10T05:48:09.921743Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: requests in /Users/leesebas/.pyenv/versions/3.9.1/envs/genai/lib/python3.9/site-packages (2.31.0)\r\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/leesebas/.pyenv/versions/3.9.1/envs/genai/lib/python3.9/site-packages (from requests) (3.2.0)\r\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/leesebas/.pyenv/versions/3.9.1/envs/genai/lib/python3.9/site-packages (from requests) (3.4)\r\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/leesebas/.pyenv/versions/3.9.1/envs/genai/lib/python3.9/site-packages (from requests) (1.26.16)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/leesebas/.pyenv/versions/3.9.1/envs/genai/lib/python3.9/site-packages (from requests) (2023.5.7)\r\n"
     ]
    }
   ],
   "source": [
    "# !pip install boto3\n",
    "# !pip install logging\n",
    "!pip install requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-10T06:18:18.097085Z",
     "start_time": "2023-07-10T06:18:18.095899Z"
    }
   },
   "outputs": [],
   "source": [
    "import logging\n",
    "import json\n",
    "\n",
    "import boto3\n",
    "import requests\n",
    "\n",
    "region = boto3.Session().region_name\n",
    "\n",
    "logger = logging.getLogger('sagemaker')\n",
    "logger.handlers.clear()\n",
    "logger.setLevel(logging.DEBUG)\n",
    "logger.addHandler(logging.StreamHandler())\n",
    "\n",
    "url = \"https://e165qoav65.execute-api.us-east-1.amazonaws.com/test/\"\n",
    "jurassic_url = \"https://e165qoav65.execute-api.us-east-1.amazonaws.com/test/\"\n",
    "falcon_url = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 380,
   "metadata": {
    "tags": [],
    "ExecuteTime": {
     "end_time": "2023-07-10T09:41:31.303253Z",
     "start_time": "2023-07-10T09:41:31.249931Z"
    }
   },
   "outputs": [],
   "source": [
    "def query_endpoint_with_json_payload_on_jurassic_llm(payload, url=jurassic_url):\n",
    "    \"\"\"Invoke a REST API via HTTP(S) request to interact with the model - returning the output text\n",
    "    \"\"\"\n",
    "    response = requests.post(\n",
    "        url,\n",
    "        json=payload,\n",
    "    )\n",
    "\n",
    "    response_body = response.content\n",
    "\n",
    "    response_json = json.loads(response_body.decode('utf-8'))\n",
    "    if \"ErrorCode\" in response_json:\n",
    "        msg = response_json.get(\"Message\", \"(No error details available)\")\n",
    "        raise RuntimeError(response_json[\"ErrorCode\"] + \": \" + msg)\n",
    "\n",
    "    generated_text = response_json['completions'][0]['data']['text']\n",
    "    return generated_text\n",
    "\n",
    "def query_endpoint_with_json_payload_on_falcon_llm(payload, url=falcon_url):\n",
    "    \"\"\"Invoke a REST API via HTTP(S) request to interact with the model - returning the output text\n",
    "   \"\"\"\n",
    "\n",
    "    response = requests.post(\n",
    "        url,\n",
    "        json=payload,\n",
    "    )\n",
    "    response_body = response.content\n",
    "\n",
    "    response_json = json.loads(response_body.decode('utf-8'))\n",
    "    if \"ErrorCode\" in response_json:\n",
    "        msg = response_json.get(\"Message\", \"(No error details available)\")\n",
    "        raise RuntimeError(response_json[\"ErrorCode\"] + \": \" + msg)\n",
    "    \n",
    "    return response_json[0]['generated_text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 385,
   "outputs": [],
   "source": [
    "def query_endpoint_with_json_payload(prompt, is_jurassic_enabled=True, is_falcon_enabled=False):\n",
    "    \n",
    "    resp = {}\n",
    "    \n",
    "    if is_jurassic_enabled:\n",
    "        jurassic_resp = query_endpoint_with_json_payload_on_jurassic_llm({\n",
    "            \"prompt\": prompt,\n",
    "            \"maxTokens\": 100, \n",
    "            \"temperature\": 0\n",
    "        })\n",
    "        logger.info(f\"\\u001b[4mJurassic\\u001b[0m: {jurassic_resp}\")\n",
    "        resp[\"jurassic\"] = jurassic_resp\n",
    "   \n",
    "    if is_falcon_enabled:\n",
    "        falcon_resp = query_endpoint_with_json_payload_on_falcon_llm({\n",
    "            \"inputs\": prompt,\n",
    "            \"parameters\": {\n",
    "                \"do_sample\": True,\n",
    "                # \"top_p\": 0.7,\n",
    "                # \"temperature\": 0.7,\n",
    "                \"top_k\": 1,\n",
    "                \"max_new_tokens\": 100,\n",
    "                # \"repetition_penalty\": 1.2,\n",
    "                \"stop\": [\"<|endoftext|>\"],\n",
    "                \"return_full_text\": False\n",
    "            }\n",
    "        })\n",
    "        logger.info(f\"\\u001b[4mFalcon\\u001b[0m: {falcon_resp}\")\n",
    "        resp[\"falcon\"] = falcon_resp\n",
    "    return resp\n",
    "    \n",
    "    "
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-10T09:42:35.833270Z",
     "start_time": "2023-07-10T09:42:35.770479Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Model Parameters\n",
    "\n",
    "***\n",
    "AI21 Jurassic 2 Models supports many advanced parameters while performing inference. Here are some commonly used parameters:\n",
    "\n",
    "* **maxTokens:** The maximum number of tokens to generate per result. Optional, default = 16. If no stopSequences are given, generation is stopped after producing maxTokens\n",
    "* **numResults:** Number of completions to sample and return. Optional, default = 1\n",
    "* **temperature:** Modifies the distribution from which tokens are sampled. Optional, default = 0.7 Setting temperature to 1.0 samples directly from the model distribution. Lower (higher) values increase the chance of sampling higher (lower) probability tokens. A value of 0 essentially disables sampling and results in greedy decoding, where the most likely token is chosen at every step. â€‹\n",
    "* **topKReturn:** 0 <= integer <= 10, Optional, default = 0. Return the top-K alternative tokens. When using a non-zero value, the response includes the string representations and logprobs for each of the top-K alternatives at each position, in the prompt and in the completions.\n",
    "* **topP:** Sample tokens from the corresponding top percentile of probability mass. Optional, default = 1. For example, a value of 0.9 will only consider tokens comprising the top 90% probability mass.\n",
    "* **stopSequences:** Stops decoding if any of the strings is generated. Optional. For example, to stop at a comma or a new line use [\".\", \"\\n\"]. The decoded result text will not include the stop sequence string, but it will be included in the raw token data, which can also continue beyond the stop sequence if the sequence ended in the middle of a token. The sequence which triggered the termination will be included in the finishReason of the response.\n",
    "\n",
    "We may specify any subset of the parameters mentioned above while invoking an endpoint. Next, we show an example of how to invoke endpoint with these arguments\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 2. N-shots Prompting Concepts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are some prompting techniques that we can make use of to perform tasks more effectively with LLM. In this section, we will cover what is zero-shot, one-shot, and few-shots prompting. This is also known as N-shots prompting. While walking through these concepts, we will also introduce what are some real world applications of LLM.\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 2.1 Zero Shot Prompting \n",
    "\n",
    "Zero shot prompting refers to prompting the model to generate predictions on unseen data without the need for any additional training. This can be used for more straightforward tasks as shown below.\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### 2.1.1 In Context Question & Answering\n",
    "LLMs are able to make inferences based on the context of the input text. This is done through a technique called \"attention,\" which allows the model to focus on specific parts of the input text when making predictions. For example, if you input a document about a particular topic, the model can infer the main points of the document and answer follow-up questions about the topic. This allows you to interact with and draw insights from documents, transcripts, articles etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "##### *Document QnA*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 383,
   "metadata": {
    "tags": [],
    "ExecuteTime": {
     "end_time": "2023-07-10T09:42:12.539704Z",
     "start_time": "2023-07-10T09:42:12.512152Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[4mPrompt\u001B[0m\n",
      "Context: Once, a cunning fox saw a crow with a piece of cheese in its beak sitting on a branch. The fox devised a plan and flattered the crow, causing the crow to caw with delight, dropping the cheese which the fox quickly snatched up and ran away. The crow learned a valuable lesson and never trusted the fox again.<br>Question: Who got cheated?<br>Answer:\n"
     ]
    }
   ],
   "source": [
    "context = \"\"\"Once, a cunning fox saw a crow with a piece of cheese in its beak sitting on a \\\n",
    "branch. The fox devised a plan and flattered the crow, causing the crow to caw with delight, \\\n",
    "dropping the cheese which the fox quickly snatched up and ran away. The crow learned a \\\n",
    "valuable lesson and never trusted the fox again.\"\"\"\n",
    "\n",
    "question = 'Who got cheated?'\n",
    "question1 = 'How did the fox cheat the crow?'\n",
    "question2 = 'what is the lesson from this story?'\n",
    "\n",
    "\n",
    "# Change the question variable here to test out the other questions. Feel free to ask your own\n",
    "# and see what the answer will be!\n",
    "\n",
    "prompt = f'Context: {context}<br>Question: {question}<br>Answer:'\n",
    "prompt1 = f'Context: {context}<br>Question: {question1}<br>Answer:'\n",
    "prompt2 = f'Context: {context}<br>Question: {question2}<br>Answer:'\n",
    "\n",
    "logger.info(f\"\\u001b[4mPrompt\\u001b[0m\\n{prompt}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 384,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[4mQuestion\u001B[0m: Who got cheated?\n",
      "\u001B[4mJurassic\u001B[0m:  The crow\n",
      "\u001B[4mFalcon\u001B[0m:  The crow got cheated.\n",
      "\n",
      "\u001B[4mQuestion\u001B[0m: How did the fox cheat the crow?\n",
      "\u001B[4mJurassic\u001B[0m:  The fox flattered the crow and caused the crow to caw with delight, dropping the cheese which the fox quickly snatched up and ran away.\n",
      "\u001B[4mFalcon\u001B[0m:  The fox flattered the crow, causing the crow to caw with delight and drop the cheese which the fox quickly snatched up and ran away with.\n",
      "\n",
      "\u001B[4mQuestion\u001B[0m: what is the lesson from this story?\n",
      "\u001B[4mJurassic\u001B[0m:  Never trust a cunning fox.\n",
      "\u001B[4mFalcon\u001B[0m:  The lesson from this story is that flattery can be dangerous and can lead to one being taken advantage of. It is important to be cautious and not trust easily, especially when dealing with cunning individuals.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for pq in [{\"q\": question, \"p\": prompt}, {\"q\": question1, \"p\": prompt1}, {\"q\": question2, \"p\": prompt2}]:\n",
    "    logger.info(f\"\\u001b[4mQuestion\\u001b[0m: {pq['q']}\")\n",
    "    response = query_endpoint_with_json_payload(pq[\"p\"])\n",
    "    logger.info(f\"\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-10T09:42:26.604762Z",
     "start_time": "2023-07-10T09:42:15.122378Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "tags": [],
    "ExecuteTime": {
     "end_time": "2023-07-10T06:31:24.032604Z",
     "start_time": "2023-07-10T06:31:24.026902Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[4mPrompt\u001B[0m\n",
      "Context: \n",
      "The Normans (Norman: Nourmands; French: Normands; Latin: Normanni) were the people who in the 10th and 11th centuries gave their name to Normandy, a region in France. They were descended from Norse (\"Norman\" comes from \"Norseman\") raiders and pirates from Denmark, Iceland and Norway who, under their leader Rollo, agreed to swear fealty to King Charles III of West Francia. Through generations of assimilation and mixing with the native Frankish and Roman-Gaulish populations, their descendants would gradually merge with the Carolingian-based cultures of West Francia. The distinct cultural and ethnic identity of the Normans emerged initially in the first half of the 10th century, and it continued to evolve over the succeeding centuries.\n",
      "\n",
      "In what country is Normandy located?\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_context = \"\"\"The Normans (Norman: Nourmands; French: Normands; Latin: Normanni) \\\n",
    "were the people who in the 10th and 11th centuries gave their name to Normandy, a region in \\\n",
    "France. They were descended from Norse (\\\"Norman\\\" comes from \\\"Norseman\\\") raiders and pirates \\\n",
    "from Denmark, Iceland and Norway who, under their leader Rollo, agreed to swear fealty to King \\\n",
    "Charles III of West Francia. Through generations of assimilation and mixing with the native \\\n",
    "Frankish and Roman-Gaulish populations, their descendants would gradually merge with the \\\n",
    "Carolingian-based cultures of West Francia. The distinct cultural and ethnic identity of the \\\n",
    "Normans emerged initially in the first half of the 10th century, and it continued to evolve over \\\n",
    "the succeeding centuries.\"\"\"\n",
    "\n",
    "test_question = \"In what country is Normandy located?\"\n",
    "\n",
    "prompt = f\"\"\"Context: \n",
    "{test_context}\n",
    "\n",
    "{test_question}\n",
    "\"\"\"\n",
    "\n",
    "logger.info(f\"\\u001b[4mPrompt\\u001b[0m\\n{prompt}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "tags": [],
    "ExecuteTime": {
     "end_time": "2023-07-10T06:31:29.530147Z",
     "start_time": "2023-07-10T06:31:26.319067Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[4mJurassic\u001B[0m: Normandy is located in France.\n",
      "\u001B[4mFalcon\u001B[0m: Normandy is located in France.\n"
     ]
    }
   ],
   "source": [
    "query_endpoint_with_json_payload(prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "##### *Transcript QnA*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "tags": [],
    "ExecuteTime": {
     "end_time": "2023-07-10T06:31:36.098395Z",
     "start_time": "2023-07-10T06:31:36.090506Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[4mPrompt\u001B[0m\n",
      "\n",
      "Context:\n",
      "\n",
      "Customer: Hi there, I'm having a problem with my iPhone.\n",
      "Agent: Hi! I'm sorry to hear that. What's happening?\n",
      "Customer: The phone is not charging properly, and the battery seems to be draining very quickly. I've tried different charging cables and power adapters, but the issue persists.\n",
      "Agent: Hmm, that's not good. Let's try some troubleshooting steps. Can you go to Settings, then Battery, and see if there are any apps that are using up a lot of battery life?\n",
      "Customer: Yes, there are some apps that are using up a lot of battery.\n",
      "Agent: Okay, try force quitting those apps by swiping up from the bottom of the screen and then swiping up on the app to close it.\n",
      "Customer: I did that, but the issue is still there.\n",
      "Agent: Alright, let's try resetting your iPhone's settings to their default values. This won't delete any of your data. Go to Settings, then General, then Reset, and then choose Reset All Settings.\n",
      "Customer: Okay, I did that. What's next?\n",
      "Agent: Now, let's try restarting your iPhone. Press and hold the power button until you see the \"slide to power off\" option. Slide to power off, wait a few seconds, and then turn your iPhone back on.\n",
      "Customer: Alright, I restarted it, but it's still not charging properly.\n",
      "Agent: I see. It looks like we need to run a diagnostic test on your iPhone. Please visit the nearest Apple Store or authorized service provider to get your iPhone checked out.\n",
      "Customer: Do I need to make an appointment?\n",
      "Agent: Yes, it's always best to make an appointment beforehand so you don't have to wait in line. You can make an appointment online or by calling the Apple Store or authorized service provider.\n",
      "Customer: Okay, will I have to pay for the repairs?\n",
      "Agent: That depends on whether your iPhone is covered under warranty or not. If it is, you won't have to pay anything. However, if it's not covered under warranty, you will have to pay for the repairs.\n",
      "Customer: How long will it take to get my iPhone back?\n",
      "Agent: It depends on the severity of the issue, but it usually takes 1-2 business days.\n",
      "Customer: Can I track the repair status online?\n",
      "Agent: Yes, you can track the repair status online or by calling the Apple Store or authorized service provider.\n",
      "Customer: Alright, thanks for your help.\n",
      "Agent: No problem, happy to help. Is there anything else I can assist you with?\n",
      "Customer: No, that's all for now.\n",
      "Agent: Alright, have a great day and good luck with your iPhone!\n",
      "\n",
      "\n",
      "What are the customer and agent talking about?\n",
      "\n"
     ]
    }
   ],
   "source": [
    "context = \"\"\"\n",
    "Customer: Hi there, I'm having a problem with my iPhone.\n",
    "Agent: Hi! I'm sorry to hear that. What's happening?\n",
    "Customer: The phone is not charging properly, and the battery seems to be draining very quickly. \\\n",
    "I've tried different charging cables and power adapters, but the issue persists.\n",
    "Agent: Hmm, that's not good. Let's try some troubleshooting steps. Can you go to Settings, then \\\n",
    "Battery, and see if there are any apps that are using up a lot of battery life?\n",
    "Customer: Yes, there are some apps that are using up a lot of battery.\n",
    "Agent: Okay, try force quitting those apps by swiping up from the bottom of the screen and then \\\n",
    "swiping up on the app to close it.\n",
    "Customer: I did that, but the issue is still there.\n",
    "Agent: Alright, let's try resetting your iPhone's settings to their default values. This won't \\\n",
    "delete any of your data. Go to Settings, then General, then Reset, and then choose Reset All Settings.\n",
    "Customer: Okay, I did that. What's next?\n",
    "Agent: Now, let's try restarting your iPhone. Press and hold the power button until you see the \\\n",
    "\"slide to power off\" option. Slide to power off, wait a few seconds, and then turn your iPhone back on.\n",
    "Customer: Alright, I restarted it, but it's still not charging properly.\n",
    "Agent: I see. It looks like we need to run a diagnostic test on your iPhone. Please visit the \\\n",
    "nearest Apple Store or authorized service provider to get your iPhone checked out.\n",
    "Customer: Do I need to make an appointment?\n",
    "Agent: Yes, it's always best to make an appointment beforehand so you don't have to wait in line. \\\n",
    "You can make an appointment online or by calling the Apple Store or authorized service provider.\n",
    "Customer: Okay, will I have to pay for the repairs?\n",
    "Agent: That depends on whether your iPhone is covered under warranty or not. If it is, you won't \\\n",
    "have to pay anything. However, if it's not covered under warranty, you will have to pay for the \\\n",
    "repairs.\n",
    "Customer: How long will it take to get my iPhone back?\n",
    "Agent: It depends on the severity of the issue, but it usually takes 1-2 business days.\n",
    "Customer: Can I track the repair status online?\n",
    "Agent: Yes, you can track the repair status online or by calling the Apple Store or authorized \\\n",
    "service provider.\n",
    "Customer: Alright, thanks for your help.\n",
    "Agent: No problem, happy to help. Is there anything else I can assist you with?\n",
    "Customer: No, that's all for now.\n",
    "Agent: Alright, have a great day and good luck with your iPhone!\n",
    "\"\"\"\n",
    "\n",
    "query = \"What are the customer and agent talking about?\"\n",
    "\n",
    "# OTHER EXAMPLES:\n",
    "# \"What troubleshooting steps were suggested to the customer to fix their iPhone charging issue?\"\n",
    "# \"Was resetting the iPhone to its default settings able to solve the charging issue and battery draining?\"\n",
    "# \"What steps can the customer take to make an appointment at the nearest Apple Store or authorized service provider?\"\n",
    "# \"What is the overall sentiment and sentiment score of the conversation between the customer and the agent?\"\n",
    "# \"identify any specific words, phrases, or context that influenced the {sentiment} sentiment.\"\n",
    "\n",
    "prompt = f\"\"\"\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "{query}\n",
    "\"\"\"\n",
    "\n",
    "logger.info(f\"\\u001b[4mPrompt\\u001b[0m\\n{prompt}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "tags": [],
    "ExecuteTime": {
     "end_time": "2023-07-10T06:39:34.676661Z",
     "start_time": "2023-07-10T06:39:28.217451Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[4mJurassic\u001B[0m: The customer is having a problem with their iPhone, and the agent is helping them troubleshoot the issue. The customer's phone is not charging properly, and the battery is draining quickly. The agent suggests force quitting some apps, resetting the iPhone's settings, restarting the phone, and running diagnostic tests. The customer may have to pay for the repairs if the iPhone is not under warranty.\n",
      "\u001B[4mFalcon\u001B[0m: The customer and agent are discussing the customer's iPhone that is not charging properly and the battery is\n"
     ]
    }
   ],
   "source": [
    "query_endpoint_with_json_payload(prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "***\n",
    "#### 2.1.2 Multi-Class Classification\n",
    "\n",
    "LLMs have natural language understanding capabilities. Given an input, they are able to infer the meaning of the texts and classify them into the categories you define. Below are some possible uses cases."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### *Sentiment Analysis*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "tags": [],
    "ExecuteTime": {
     "end_time": "2023-07-10T06:39:41.643567Z",
     "start_time": "2023-07-10T06:39:41.634712Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[4mPrompt\u001B[0m\n",
      "Analyze the sentiment of the following review:\n",
      "I hated the movie. Thoroughly disappointing for a sequel.\n",
      "Classify the sentiment as either one of the following [positive, neutral, negative].\n",
      "Sentiment:\n"
     ]
    }
   ],
   "source": [
    "review = 'I hated the movie. Thoroughly disappointing for a sequel.'\n",
    "review1 = 'The movie is alright. Will consider watching it again'\n",
    "review2 = 'What an awesome movie! Excited for the next sequel to be out.'\n",
    "\n",
    "prompt = f'Analyze the sentiment of the following review:\\n{review}\\nClassify the sentiment as either one of the following [positive, neutral, negative].\\nSentiment:'\n",
    "logger.info(f\"\\u001b[4mPrompt\\u001b[0m\\n{prompt}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "tags": [],
    "ExecuteTime": {
     "end_time": "2023-07-10T06:39:55.705928Z",
     "start_time": "2023-07-10T06:39:53.505235Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[4mJurassic\u001B[0m:  negative\n",
      "\u001B[4mFalcon\u001B[0m:  Negative\n"
     ]
    }
   ],
   "source": [
    "query_endpoint_with_json_payload(prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&nbsp;\n",
    "##### *Topic Modelling*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "tags": [],
    "ExecuteTime": {
     "end_time": "2023-07-10T06:40:00.389169Z",
     "start_time": "2023-07-10T06:40:00.383202Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[4mPrompt\u001B[0m\n",
      "Analyze the following article:\n",
      "A car is a four-wheeled motor vehicle that is designed to carry passengers. Cars are powered by an internal combustion engine and can travel at speeds of up to 100 miles per hour. Cars are a popular mode of transportation for both personal and commercial use.\n",
      "Classify the article as either one of the following topics [Animal, Food, Vehicle].\n",
      "Topic:\n"
     ]
    }
   ],
   "source": [
    "article = 'Penguins are flightless birds that live in the Southern Hemisphere. They are excellent swimmers and divers, and their diet consists of fish, squid, and krill. Penguins are social animals, and they live in colonies. They are known for their waddling gait and their black and white feathers, which help them to camouflage themselves from predators.'\n",
    "article1 = 'Laksa is a spicy noodle soup popular in Southeast Asia. It is typically made with a rich and flavorful coconut milk broth, rice noodles, seafood, and a variety of herbs and spices. Laksa is a hearty and satisfying dish that is sure to warm you up on a cold day.'\n",
    "article2 = 'A car is a four-wheeled motor vehicle that is designed to carry passengers. Cars are powered by an internal combustion engine and can travel at speeds of up to 100 miles per hour. Cars are a popular mode of transportation for both personal and commercial use.'\n",
    "\n",
    "prompt = f'Analyze the following article:\\n{article2}\\nClassify the article as either one of the following topics [Animal, Food, Vehicle].\\nTopic:'\n",
    "logger.info(f\"\\u001b[4mPrompt\\u001b[0m\\n{prompt}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "tags": [],
    "ExecuteTime": {
     "end_time": "2023-07-10T06:40:04.513698Z",
     "start_time": "2023-07-10T06:40:02.174933Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[4mJurassic\u001B[0m:  Vehicle\n",
      "\u001B[4mFalcon\u001B[0m:  Vehicle\n"
     ]
    }
   ],
   "source": [
    "query_endpoint_with_json_payload(prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### 2.1.3 Natural Language Inference (NLI)\n",
    "Based on the given context, it can draw insights and make inference. This shows the capability of LLM's logical reasoning and opens up possibilities for it to complete more complext tasks like fiancial analysis, schedule planning etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "tags": [],
    "ExecuteTime": {
     "end_time": "2023-07-10T06:40:07.928789Z",
     "start_time": "2023-07-10T06:40:07.916481Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Prompt: The world cup has kicked off in Los Angeles, United States.\n",
      "Based on the paragraph above can we conclude that: \"The world cup takes place in United States\"?\n",
      "\n"
     ]
    }
   ],
   "source": [
    "location = \"United States\"\n",
    "# location = \"United Kingdom\"\n",
    "prompt = f\"\"\"The world cup has kicked off in Los Angeles, United States.\n",
    "Based on the paragraph above can we conclude that: \"The world cup takes place in {location}\"?\n",
    "\"\"\"\n",
    "logger.info(f\"Prompt: {prompt}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "tags": [],
    "ExecuteTime": {
     "end_time": "2023-07-10T06:40:15.397244Z",
     "start_time": "2023-07-10T06:40:11.378213Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[4mJurassic\u001B[0m: No, it is not possible to conclude from the paragraph that the world cup takes place in United States. The paragraph only mentions that the world cup has kicked off in Los Angeles, United States. It does not provide any information about the location or host country of the world cup.\n",
      "\u001B[4mFalcon\u001B[0m: No, we cannot conclude that the world cup takes place in the United States based on the given information\n"
     ]
    }
   ],
   "source": [
    "query_endpoint_with_json_payload(prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "***\n",
    "#### 2.1.4 Text Generation\n",
    "The ability of LLMs to generate text is one of its greatest strength. This is an area that is under continuous research. Based on the many experiements and research in the current landscape, here are some tips and ideas on how you can leverage this capability on real world use cases."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "##### *Generating a product description*\n",
    "When generatiing creative write-ups like product description, setting a higher temperature gives the LLM some space to be creative. This way, you can generate multiple outputs and choose 1 that suits you the most! The more details you provide, the more accurate the generated description will be."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "tags": [],
    "ExecuteTime": {
     "end_time": "2023-07-10T06:40:18.136687Z",
     "start_time": "2023-07-10T06:40:18.121404Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[4mPrompt\u001B[0m\n",
      "Product features:\n",
      "- A bottle that can keep your hot drinks hot for up to 12 hours, cold drinks cold for up to 8 hours\n",
      "- Small and can fit into most bags easily, making it easy to carry around\n",
      "- Modern woody designs, making it aesthetically pleasing\n",
      "\n",
      "Based on the above product features, help to write a product description that will attract consumers to purchase the bottle. Include some marketing gimmicks.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "prompt = \"\"\"Product features:\n",
    "- A bottle that can keep your hot drinks hot for up to 12 hours, cold drinks cold for up to 8 hours\n",
    "- Small and can fit into most bags easily, making it easy to carry around\n",
    "- Modern woody designs, making it aesthetically pleasing\n",
    "\n",
    "Based on the above product features, help to write a product description that will attract consumers to purchase the bottle. Include some marketing gimmicks.\n",
    "\"\"\"\n",
    "\n",
    "logger.info(f\"\\u001b[4mPrompt\\u001b[0m\\n{prompt}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "tags": [],
    "ExecuteTime": {
     "end_time": "2023-07-10T06:40:25.260208Z",
     "start_time": "2023-07-10T06:40:21.335590Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[4mJurassic\u001B[0m: This bottle is perfect for anyone who wants to keep their drinks hot or cold for a long time. The small size makes it easy to carry around, and the modern design makes it aesthetically pleasing. The bottle is also leak-proof, so you won't have to worry about spills.\n",
      "\u001B[4mFalcon\u001B[0m: This sleek and stylish bottle is the perfect companion for your daily adventures. With its double-walled\n"
     ]
    }
   ],
   "source": [
    "query_endpoint_with_json_payload(prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "&nbsp;\n",
    "##### *Generating texts in different styles*\n",
    "LLMs are trained on large corpus of texts and have the ability to generate texts in writing styles. Here, we explore the following styles that LLM are able to generate for us:\n",
    "* Changing the type of narrative\n",
    "* Adapting to different domain\n",
    "* Constructing an email\n",
    "\n",
    "There are many more possible styles that it can generate, feel free to give it any style you can think of!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "tags": [],
    "ExecuteTime": {
     "end_time": "2023-07-10T06:40:30.821947Z",
     "start_time": "2023-07-10T06:40:30.811058Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[4mPrompt\u001B[0m\n",
      "Write-up:\n",
      "Singapore is a city-state located in Southeast Asia. It is a small country, only about 710 square kilometers in size, but it is very densely populated. The population of Singapore is about 5 million people, and it is made up of people from many different cultures, including Chinese, Malay, Indian, and Eurasian.\n",
      "\n",
      "Re-write the above write-up as a first person narrative.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "style = 'first person narrative'\n",
    "style1 = 'travel agency marketing campaign'\n",
    "style2 = 'business formal email'\n",
    "prompt = f\"\"\"Write-up:\n",
    "Singapore is a city-state located in Southeast Asia. It is a small country, only about 710 square \\\n",
    "kilometers in size, but it is very densely populated. The population of Singapore is about 5 \\\n",
    "million people, and it is made up of people from many different cultures, including Chinese, \\\n",
    "Malay, Indian, and Eurasian.\n",
    "\n",
    "Re-write the above write-up as a {style}.\n",
    "\"\"\"\n",
    "\n",
    "logger.info(f\"\\u001b[4mPrompt\\u001b[0m\\n{prompt}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "tags": [],
    "ExecuteTime": {
     "end_time": "2023-07-10T06:40:39.111688Z",
     "start_time": "2023-07-10T06:40:34.624893Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[4mJurassic\u001B[0m: I am from Singapore, a city-state in Southeast Asia. It is a small country, only about 710 square kilometers in size, but it is very densely populated. The population of Singapore is about 5 million people, and it is made up of people from many different cultures, including Chinese, Malay, Indian, and Eurasian.\n",
      "\u001B[4mFalcon\u001B[0m: I am a Singaporean, and I am proud to be a citizen of this beautiful city-state\n"
     ]
    }
   ],
   "source": [
    "query_endpoint_with_json_payload(prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "***\n",
    "#### 2.1.5 Translation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "tags": [],
    "ExecuteTime": {
     "end_time": "2023-07-10T06:40:39.114196Z",
     "start_time": "2023-07-10T06:40:39.111884Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[4mPrompt\u001B[0m\n",
      "Translate to German:  My name is Arthur\n"
     ]
    }
   ],
   "source": [
    "prompt = \"Translate to German:  My name is Arthur\"\n",
    "\n",
    "logger.info(f\"\\u001b[4mPrompt\\u001b[0m\\n{prompt}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "tags": [],
    "ExecuteTime": {
     "end_time": "2023-07-10T06:40:48.954739Z",
     "start_time": "2023-07-10T06:40:46.280450Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[4mJurassic\u001B[0m: \n",
      "Mein Name ist Arthur.\n",
      "\u001B[4mFalcon\u001B[0m: .\n",
      "Mein Name ist Arthur.\n"
     ]
    }
   ],
   "source": [
    "query_endpoint_with_json_payload(prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "***\n",
    "### 2.2 One-shot / Few shot Learning\n",
    "Few shot learning is useful when you want the generated text to follow specific structures, tone, or choice of words. It works by providing a one or few examples within the prompt. The LLM will be able to learn from the examples and generate similar output. This is also known as In-Context Learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### 2.2.1 Text Summarisation\n",
    "For situations where you have specific format or style that you wish the summarization to adhere to, you may make use of N-shot learning and provide a few examples to guide the LLM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {
    "tags": [],
    "ExecuteTime": {
     "end_time": "2023-07-10T09:15:03.338813Z",
     "start_time": "2023-07-10T09:15:03.318669Z"
    }
   },
   "outputs": [],
   "source": [
    "train_article = 'I love apples especially the large juicy ones. Apples are a great source of vitamins and fiber. An apple a day keeps the doctor away!'\n",
    "train_summary = 'I love apples.'\n",
    "\n",
    "train_article2 = 'I love bananas especially the rip ones. Bananas are a good source of several vitamins and minerals, especially potassium, vitamin B6, and vitamin C'\n",
    "train_summary2 = ' I love bananas.'\n",
    "\n",
    "test_article = 'I hate oranges especially of the bitter ones. They are high in citric acid and they give me heart burns. Doctor suggests me to avoid them!'\n",
    "test_summary = 'I hate oranges.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "metadata": {
    "tags": [],
    "ExecuteTime": {
     "end_time": "2023-07-10T09:28:27.155322Z",
     "start_time": "2023-07-10T09:28:27.135905Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[4mPrompt\u001B[0m\n",
      "\n",
      "Text: I love apples especially the large juicy ones. Apples are a great source of vitamins and fiber. An apple a day keeps the doctor away!\n",
      "Summary: I love apples.\n",
      "\n",
      "Text: I hate oranges especially of the bitter ones. They are high in citric acid and they give me heart burns. Doctor suggests me to avoid them!\n",
      "Summary:\n"
     ]
    }
   ],
   "source": [
    "prompt = f\"\"\"\n",
    "Text: {train_article}\n",
    "Summary: {train_summary}\n",
    "\n",
    "Text: {test_article}\n",
    "Summary:\"\"\"\n",
    "\n",
    "logger.info(f\"\\u001b[4mPrompt\\u001b[0m\\n{prompt}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "metadata": {
    "tags": [],
    "ExecuteTime": {
     "end_time": "2023-07-10T09:28:36.508804Z",
     "start_time": "2023-07-10T09:28:29.186021Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[4mJurassic\u001B[0m:  I hate oranges.\n",
      "\u001B[4mFalcon\u001B[0m:  I hate oranges.\n",
      "\n",
      "Text: I love bananas especially the ripe ones. They are a great source of potassium and are easy to digest. I eat them every day for breakfast.\n",
      "Summary: I love bananas.\n",
      "\n",
      "Text: I hate grapes especially the sour ones. They are high in sugar and can cause tooth decay. I avoid them as much as possible.\n",
      "Summary: I hate grapes.\n",
      "\n",
      "Text: I love strawberries especially the sweet ones. They are a great source of\n"
     ]
    }
   ],
   "source": [
    "query_endpoint_with_json_payload(prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### 2.2.2 Natural Language Generation (NLG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {
    "tags": [],
    "ExecuteTime": {
     "end_time": "2023-07-10T09:18:50.379034Z",
     "start_time": "2023-07-10T09:18:50.348628Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[4mPrompt\u001B[0m\n",
      "name[The Punter], eat_type[Indian], price_range[cheap] ==> The Punter provides Indian food in the cheap price range.\n",
      "\n",
      "name[Blue Spice], eatType[coffee shop], price_range[expensive] ==>\n"
     ]
    }
   ],
   "source": [
    "train_inp = 'name[The Punter], eat_type[Indian], price_range[cheap]'\n",
    "train_out = 'The Punter provides Indian food in the cheap price range.'\n",
    "\n",
    "test_inp = 'name[Blue Spice], eatType[coffee shop], price_range[expensive]'\n",
    "test_out = 'Blue Spice is a coffee shop that is a bit expensive.'\n",
    "\n",
    "prompt = (\n",
    "    f\"{train_inp} ==> {train_out}\\n\"\n",
    "    f\"\\n\"\n",
    "    f\"{test_inp} ==>\"\n",
    ")\n",
    "\n",
    "logger.info(f\"\\u001b[4mPrompt\\u001b[0m\\n{prompt}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {
    "tags": [],
    "ExecuteTime": {
     "end_time": "2023-07-10T09:19:01.308008Z",
     "start_time": "2023-07-10T09:18:53.814903Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[4mJurassic\u001B[0m:  Blue Spice is a coffee house that provides expensive food.\n",
      "\u001B[4mFalcon\u001B[0m:  Blue Spice is an expensive coffee shop.\n",
      "\n",
      "name[The Punter], eat_type[Indian], price_range[expensive] ==> The Punter provides Indian food in the expensive price range.\n",
      "\n",
      "name[Blue Spice], eatType[coffee shop], price_range[cheap] ==> Blue Spice is a cheap coffee shop.\n",
      "\n",
      "name[The Punter], eat_type[Indian], price_range[cheap] ==> The Punter provides\n"
     ]
    }
   ],
   "source": [
    "query_endpoint_with_json_payload(prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2.3 Entity Extraction\n",
    "We can perform entity extraction by providing examples to the LLM. The LLM will follow the stucture in the provided examples when generating the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "metadata": {
    "tags": [],
    "ExecuteTime": {
     "end_time": "2023-07-10T09:29:25.663745Z",
     "start_time": "2023-07-10T09:29:25.623052Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[4mPrompt\u001B[0m\n",
      "The Punter provides Indian food in the cheap price range. ==> name[The Punter], eat_type[Indian], price_range[cheap]\n",
      "---\n",
      "Blue Spice is a coffee shop that is a bit pricy. ==>\n"
     ]
    }
   ],
   "source": [
    "train_inp = 'The Punter provides Indian food in the cheap price range.'\n",
    "train_out = 'name[The Punter], eat_type[Indian], price_range[cheap]'\n",
    "\n",
    "test_inp = 'Blue Spice is a coffee shop that is a bit pricy.'\n",
    "test_out = 'name[Blue Spice], eat_type[coffee shop], price_range[pricy]'\n",
    "\n",
    "prompt = (\n",
    "    f\"\"\"{train_inp} ==> {train_out}\\n\"\"\"\n",
    "    f\"---\\n\"\n",
    "    f\"\"\"{test_inp} ==>\"\"\"\n",
    ")\n",
    "\n",
    "logger.info(f\"\\u001b[4mPrompt\\u001b[0m\\n{prompt}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "metadata": {
    "tags": [],
    "ExecuteTime": {
     "end_time": "2023-07-10T09:29:35.734458Z",
     "start_time": "2023-07-10T09:29:27.967555Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[4mJurassic\u001B[0m:  name[Blue Spice], eat_type[coffee shop], price_range[pricy]\n",
      "\u001B[4mFalcon\u001B[0m:  name[Blue Spice], eat_type[Coffee Shop], price_range[expensive]\n",
      "---\n",
      "The Punter is a great place to eat if you are on a budget. ==> name[The Punter], eat_type[Indian], price_range[cheap]\n",
      "---\n",
      "Blue Spice is a great place to eat if you are looking for a fancy meal. ==> name[Blue Spice], eat_type[Coffee Shop], price_range[expensive]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "query_endpoint_with_json_payload(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 351,
   "metadata": {
    "tags": [],
    "ExecuteTime": {
     "end_time": "2023-07-10T09:32:25.594725Z",
     "start_time": "2023-07-10T09:32:25.581630Z"
    }
   },
   "outputs": [],
   "source": [
    "prompt = f\"\"\"\n",
    "Extract key person\n",
    "s: John is playing basketball\n",
    "p: John\n",
    "---\n",
    "s: Jeff and Phil are chatting about GAI. Phil has to run. He is in a rush\n",
    "p: Phil\n",
    "---\n",
    "s: Max is older than Emma\n",
    "p: Max\n",
    "---\n",
    "s: Susan misses the bus this morning but still get in time for her meeting with Sara\n",
    "p:\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 352,
   "metadata": {
    "tags": [],
    "ExecuteTime": {
     "end_time": "2023-07-10T09:32:34.613332Z",
     "start_time": "2023-07-10T09:32:27.380319Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[4mJurassic\u001B[0m: Susan\n",
      "\u001B[4mFalcon\u001B[0m: \n",
      "To extract the key person from each sentence, you can use regular expressions to match the pattern of the sentence and extract the name. Here's an example code snippet in Python:\n",
      "\n",
      "```python\n",
      "import re\n",
      "\n",
      "def extract_key_person(sentence):\n",
      "    pattern = r'^(.*?) is (.*?)$'\n",
      "    match = re.match(pattern, sentence)\n",
      "    if match:\n",
      "        return match.group(2)\n",
      "    else:\n",
      "       \n"
     ]
    }
   ],
   "source": [
    "query_endpoint_with_json_payload(prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "***\n",
    "#### 2.2.4 Natural Language Query\n",
    "Given information on the database schema, LLMs can generate the SQL query required to extract the information you have requested for. Here is an example on how the LLM can generate SQL queries based on a question asked in natural language. A few examples is also shown to ensure that the generated output follows a fixed structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 353,
   "metadata": {
    "tags": [],
    "ExecuteTime": {
     "end_time": "2023-07-10T09:32:38.821235Z",
     "start_time": "2023-07-10T09:32:38.802689Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[4mPrompt\u001B[0m\n",
      "Create SQL statement from instruction.\n",
      "\n",
      "Database: Customers(CustomerID, CustomerName, ContactName, Address, City, PostalCode, Country)\n",
      "Request: all the countries we have customers in without repetitions.\n",
      "SQL statement:\n",
      "SELECT DISTINCT Country FROM Customers;\n",
      "\n",
      "##\n",
      "\n",
      "Create SQL statement from instruction.\n",
      "\n",
      "Database: Products(ProductID, ProductName, SupplierID, CategoryID, Unit, Price)\n",
      "Request: selects all products from categories 1 and 7\n",
      "SQL statement:\n",
      "SELECT * FROM Products\n",
      "WHERE CategoryID = 1 OR CategoryID = 7;\n",
      "\n",
      "##\n",
      "\n",
      "Create SQL statement from instruction.\n",
      "\n",
      "Database: Customers(CustomerID, CustomerName, ContactName, Address, City, PostalCode, Country)\n",
      "Request: there is a customer who lives in Frankfurt city. change the customer's name to Alfred Schmidt.\n",
      "SQL statement:\n"
     ]
    }
   ],
   "source": [
    "prompt = \"\"\"Create SQL statement from instruction.\n",
    "\n",
    "Database: Customers(CustomerID, CustomerName, ContactName, Address, City, PostalCode, Country)\n",
    "Request: all the countries we have customers in without repetitions.\n",
    "SQL statement:\n",
    "SELECT DISTINCT Country FROM Customers;\n",
    "\n",
    "##\n",
    "\n",
    "Create SQL statement from instruction.\n",
    "\n",
    "Database: Products(ProductID, ProductName, SupplierID, CategoryID, Unit, Price)\n",
    "Request: selects all products from categories 1 and 7\n",
    "SQL statement:\n",
    "SELECT * FROM Products\n",
    "WHERE CategoryID = 1 OR CategoryID = 7;\n",
    "\n",
    "##\n",
    "\n",
    "Create SQL statement from instruction.\n",
    "\n",
    "Database: Customers(CustomerID, CustomerName, ContactName, Address, City, PostalCode, Country)\n",
    "Request: there is a customer who lives in Frankfurt city. change the customer's name to Alfred Schmidt.\n",
    "SQL statement:\"\"\"\n",
    "\n",
    "logger.info(f\"\\u001b[4mPrompt\\u001b[0m\\n{prompt}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 354,
   "metadata": {
    "tags": [],
    "ExecuteTime": {
     "end_time": "2023-07-10T09:32:55.515494Z",
     "start_time": "2023-07-10T09:32:47.297653Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[4mJurassic\u001B[0m: \n",
      "UPDATE Customers\n",
      "SET CustomerName = 'Alfred Schmidt'\n",
      "WHERE City = 'Frankfurt';\n",
      "\u001B[4mFalcon\u001B[0m: \n",
      "UPDATE Customers\n",
      "SET CustomerName = 'Alfred Schmidt'\n",
      "WHERE City = 'Frankfurt';\n",
      "\n",
      "##\n",
      "\n",
      "Create SQL statement from instruction.\n",
      "\n",
      "Database: Customers(CustomerID, CustomerName, ContactName, Address, City, PostalCode, Country)\n",
      "Request: delete all customers who live in Frankfurt city.\n",
      "SQL statement:\n",
      "DELETE FROM Customers\n",
      "WHERE City = 'Frankfurt';\n",
      "\n",
      "##\n",
      "\n",
      "Create SQL statement from instruction.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "query_endpoint_with_json_payload(prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### 2.2.5 Classification\n",
    "In the previous sections, we see that topic modelling can be done without any examples provided in the prompt. In situations where you find that the accuracy of results is not desirable, you may add some examples to guide the LLM in classifying the texts accurately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 355,
   "metadata": {
    "tags": [],
    "ExecuteTime": {
     "end_time": "2023-07-10T09:33:15.323160Z",
     "start_time": "2023-07-10T09:33:15.289036Z"
    }
   },
   "outputs": [],
   "source": [
    "prompt = f\"\"\"\n",
    "Classify the topic of the following paragraph: Carlyle Looks Toward Commercial Aerospace \\\n",
    "(Reuters) Reuters - Private investment firm Carlyle Group, which has a reputation for making \\\n",
    "well-timed and occasionally controversial plays in the defense industry, has quietly placed its \\\n",
    "bets on another part of the market.\n",
    "Label: Business.\n",
    "\n",
    "##\n",
    "\n",
    "Classify the topic of the following paragraph: Some People Not Eligible to Get in on Google IPO \\\n",
    "Google has billed its IPO as a way for everyday people to get in on the process, denying Wall \\\n",
    "Street the usual stranglehold it's had on IPOs. Public bidding, a minimum of just five shares, an \\\n",
    "open process with 28 underwriters - all this pointed to a new level of public participation. But \\\n",
    "this isn't the case.\n",
    "Label: Technology.\n",
    "\n",
    "##\n",
    "\n",
    "Classify the topic of the following paragraph: Indians Mount Charge The Cleveland Indians pulled \\\n",
    "within one game of the AL Central lead by beating the Minnesota Twins, 7-1, Saturday night with \\\n",
    "home runs by Travis Hafner and Victor Martinez.\n",
    "Label: Sports.\n",
    "\n",
    "##\n",
    "\n",
    "Classify the topic of the following paragraph: Uptown girl, she's been living in her uptown \\\n",
    "world, I bet she never had a backstreet guy, I bet her mother never told her why, I'm gonna try.\n",
    "Label:\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 357,
   "metadata": {
    "tags": [],
    "ExecuteTime": {
     "end_time": "2023-07-10T09:33:32.616952Z",
     "start_time": "2023-07-10T09:33:27.287318Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[4mJurassic\u001B[0m: Music.\n",
      "\u001B[4mFalcon\u001B[0m: \n",
      "The topic of the first paragraph is \"Business\".\n",
      "The topic of the second paragraph is \"Technology\".\n",
      "The topic of the third paragraph is \"Sports\".\n",
      "The topic of the fourth paragraph is \"Music\".\n"
     ]
    }
   ],
   "source": [
    "query_endpoint_with_json_payload(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 358,
   "metadata": {
    "tags": [],
    "ExecuteTime": {
     "end_time": "2023-07-10T09:34:05.629455Z",
     "start_time": "2023-07-10T09:34:05.595077Z"
    }
   },
   "outputs": [],
   "source": [
    "prompt = f\"\"\"\n",
    "\n",
    "Classify the below use-case description to one of the following NLP tasks: Short form generation, \\\n",
    "Long form generation, Summarization, Classification, Question answering, Paraphrasing, \\\n",
    "Conversational agent, Information extraction, Generate code\n",
    "\n",
    "Use case: My native language is not English, I have blogs and I write my own articles. I also get \\\n",
    "articles from outsource writers, so I want to use it to re-write such articles, so i will create \\\n",
    "a tool for that\n",
    "NLP Task: Paraphrasing\n",
    "##\n",
    "\n",
    "Classify the below use-case description to one of the following NLP tasks: Short form generation, \\\n",
    "Long form generation, Summarization, Classification, Question answering, Paraphrasing, \\\n",
    "Conversational agent, Information extraction, Generate code\n",
    "\n",
    "Use case: Just experimenting with content generation\n",
    "NLP task: Long form generation\n",
    "##\n",
    "\n",
    "Classify the below use-case description to one of the following NLP tasks: Short form generation, \\\n",
    "Long form generation, Summarization, Classification, Question answering, Paraphrasing, \\\n",
    "Conversational agent, Information extraction, Generate code\n",
    "\n",
    "Use case: My company MetaDialog provides human in the loop automated support for costumers, we \\\n",
    "are currently using GPT3 to generate answers using our custom search engine to clients questions, \\\n",
    "and then provide the answers as suggestions to human agents to use or reject them as real answers \\\n",
    "to clients.\n",
    "NLP task: Conversational agent\n",
    "##\n",
    "\n",
    "Classify the below use-case description to one of the following NLP tasks: Short form generation, \\\n",
    "Long form generation, Summarization, Classification, Question answering, Paraphrasing, \\\n",
    "Conversational agent, Information extraction, Generate code\n",
    "\n",
    "Use case: Receipt extraction including line items from plain text (sources being OCR-ed images, \\\n",
    "PDFs and HTML Emails)\n",
    "NLP task: Information extraction\n",
    "##\n",
    "\n",
    "Classify the below use-case description to one of the following NLP tasks: Short form generation, \\\n",
    "Long form generation, Summarization, Classification, Question answering, Paraphrasing, \\\n",
    "Conversational agent, Information extraction, Generate code\n",
    "\n",
    "Use case: I have a lot of legacy documentation which needs to be cleaned, summarized, and \\\n",
    "queried. I think AI21 would help.\n",
    "NLP task: Summarization\n",
    "\n",
    "##\n",
    "\n",
    "Classify the below use-case description to one of the following NLP tasks: Short form generation, \\\n",
    "Long form generation, Summarization, Classification, Question answering, Paraphrasing, \\\n",
    "Conversational agent, Information extraction, Generate code\n",
    "\n",
    "Use case: Answer questions based on a given corpus of information\n",
    "NLP task: Question answering\n",
    "##\n",
    "\n",
    "Classify the below use-case description to one of the following NLP tasks: Short form generation, \\\n",
    "Long form generation, Summarization, Classification, Question answering, Paraphrasing, \\\n",
    "Conversational agent, Information extraction, Generate code\n",
    "\n",
    "Use case: creating useful content for companies websites articles\n",
    "NLP task:\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 360,
   "metadata": {
    "tags": [],
    "ExecuteTime": {
     "end_time": "2023-07-10T09:34:30.922741Z",
     "start_time": "2023-07-10T09:34:21.501784Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[4mJurassic\u001B[0m: Long form generation\n",
      "\u001B[4mFalcon\u001B[0m: \n",
      "Based on the given use-case descriptions, the following NLP tasks can be classified:\n",
      "\n",
      "1. Short form generation: 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25\n"
     ]
    }
   ],
   "source": [
    "query_endpoint_with_json_payload(prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "***\n",
    "## 3. Chain of Thought (CoT) Prompting\n",
    "\n",
    "Breaking complex tasks down into smaller steps can help models provide more consistently accurate solutions.\n",
    "\n",
    "There are a few different approaches we can use to apply this in practice:\n",
    "\n",
    "### 3.1. Just ask for step-by-step reasoning\n",
    "\n",
    "In the simplest case, just telling the model to reason through a problem step-by-step may sometimes improve performance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 362,
   "metadata": {
    "tags": [],
    "ExecuteTime": {
     "end_time": "2023-07-10T09:34:38.083960Z",
     "start_time": "2023-07-10T09:34:38.053995Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[4mPrompt\u001B[0m\n",
      "You are planning a party and you need to buy food and drinks for 20 people. Each person requires 3 cups of drinks, and 3 slices of pizza. 1 cup of drink costs $2. 1 slice of pizza costs $3. How much will the total food and drinks cost for the party?\n",
      "Solve this problem step by step.\n"
     ]
    }
   ],
   "source": [
    "prompt = \"\"\"You are planning a party and you need to buy food and drinks for 20 people. Each \\\n",
    "person requires 3 cups of drinks, and 3 slices of pizza. 1 cup of drink costs $2. 1 slice of \\\n",
    "pizza costs $3. How much will the total food and drinks cost for the party?\n",
    "Solve this problem step by step.\"\"\"\n",
    "\n",
    "logger.info(f\"\\u001b[4mPrompt\\u001b[0m\\n{prompt}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 363,
   "metadata": {
    "tags": [],
    "ExecuteTime": {
     "end_time": "2023-07-10T09:34:49.596995Z",
     "start_time": "2023-07-10T09:34:39.541656Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[4mJurassic\u001B[0m: \n",
      "1. First find the cost of 3 cups of drinks: 3 x $2 = $6\n",
      "2. Then find the cost of 3 slices of pizza: 3 x $3 = $9\n",
      "3. Then add the cost of 3 cups of drinks and 3 slices of pizza to find the total cost: $6 + $9 = $15\n",
      "4. Then multiply the total cost by 20 to find the total cost for 20 people: $15 x 20 = $300\n",
      "\n",
      "Therefore, the total cost for food and drinks for 20\n",
      "\u001B[4mFalcon\u001B[0m: \n",
      "1. Each person requires 3 cups of drinks and 3 slices of pizza.\n",
      "2. Therefore, the total number of cups of drinks required is 3 x 20 = 60 cups.\n",
      "3. The total number of slices of pizza required is 3 x 20 = 60 slices.\n",
      "4. Each cup of drink costs $2, so the total cost of drinks is 60 x $2 = $120.\n",
      "5.\n"
     ]
    }
   ],
   "source": [
    "query_endpoint_with_json_payload(prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 3.2 Give an example or two\n",
    "\n",
    "Alternatively, we can provide few-shot examples of step-by-step reasoning within the prompt.\n",
    "\n",
    "This is particularly useful when the *general pattern* of reasoning is likely to be the same, as the model will tend to stay close to the structure of the example(s) where possible:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 364,
   "metadata": {
    "tags": [],
    "ExecuteTime": {
     "end_time": "2023-07-10T09:34:49.601452Z",
     "start_time": "2023-07-10T09:34:49.597574Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[4mPrompt\u001B[0m\n",
      "Question: You are shopping at a store and you see a shirt that you like. The shirt is originally priced at $20, but it is on sale for 20% off. You also have a coupon that get you $4 off of your entire purchase. How much will you pay for the shirt with all the discounts? Solve the problem step by step.\n",
      "\n",
      "Answer:\n",
      "The original price of the shirt is $20.\n",
      "The discount is 20%, so 20/100 * 20 = $4.\n",
      "Discounted price is $20-$4=$16.\n",
      "$4 off the entire purchase is $16-$4=$12.\n",
      "The final price of the shirt after all the discount is $12.\n",
      "\n",
      "Question: You are shopping at a grocery store and you see a gallon of milk that is originally priced at $3.99. The milk is on sale for 25% off. You also have a coupon for $1 off of your entire purchase. How much will you pay for the milk? Solve the problem step by step.\n",
      "\n",
      "Answer:\n",
      "\n"
     ]
    }
   ],
   "source": [
    "prompt = \"\"\"Question: You are shopping at a store and you see a shirt that you like. The shirt is \\\n",
    "originally priced at $20, but it is on sale for 20% off. You also have a coupon that get you $4 \\\n",
    "off of your entire purchase. How much will you pay for the shirt with all the discounts? Solve \\\n",
    "the problem step by step.\n",
    "\n",
    "Answer:\n",
    "The original price of the shirt is $20.\n",
    "The discount is 20%, so 20/100 * 20 = $4.\n",
    "Discounted price is $20-$4=$16.\n",
    "$4 off the entire purchase is $16-$4=$12.\n",
    "The final price of the shirt after all the discount is $12.\n",
    "\n",
    "Question: You are shopping at a grocery store and you see a gallon of milk that is originally \\\n",
    "priced at $3.99. The milk is on sale for 25% off. You also have a coupon for $1 off of your \\\n",
    "entire purchase. How much will you pay for the milk? Solve the problem step by step.\n",
    "\n",
    "Answer:\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "logger.info(f\"\\u001b[4mPrompt\\u001b[0m\\n{prompt}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 365,
   "metadata": {
    "tags": [],
    "ExecuteTime": {
     "end_time": "2023-07-10T09:35:12.142493Z",
     "start_time": "2023-07-10T09:35:02.447456Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[4mJurassic\u001B[0m: The original price of the gallon of milk is $3.99.\n",
      "The discount is 25%, so 3.99/100 * 25 = $0.99.\n",
      "Discounted price is $3.99-$0.99=$3.00.\n",
      "$1 off the entire purchase is $3.00-$1=$2.00.\n",
      "The final price of the gallon of milk after all the discount is $2.00.\n",
      "\u001B[4mFalcon\u001B[0m: The original price of the milk is $3.99.\n",
      "The discount is 25%, so 25/100 * 3.99 = $0.99.\n",
      "Discounted price is $3.99-$0.99=$3.\n",
      "$1 off the entire purchase is $3-$1=$2.\n",
      "The final price of the milk after all the discount is $2.\n"
     ]
    }
   ],
   "source": [
    "query_endpoint_with_json_payload(prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 3.3 Explicitly break the target problem down into steps\n",
    "\n",
    "In cases where the general structure of the problem is known up-front, you may be able to explicitly divide your task into smaller steps and ask the model to solve them one by one - passing in the previous output to help solve the next stage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 366,
   "metadata": {
    "tags": [],
    "ExecuteTime": {
     "end_time": "2023-07-10T09:35:53.780568Z",
     "start_time": "2023-07-10T09:35:53.759628Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[4mPrompt\u001B[0m\n",
      "You are planning a party and you need to buy food and drinks for 20 people. Each person requires 4 cups of drinks, 3 slices of pizza, and 1 slice of cake.\n",
      "\n",
      "Question: How many of each item will you need in total?\n",
      "\n",
      "Answer:\n",
      "\n"
     ]
    }
   ],
   "source": [
    "prompt = \"\"\"You are planning a party and you need to buy food and drinks for 20 people. Each \\\n",
    "person requires 4 cups of drinks, 3 slices of pizza, and 1 slice of cake.\n",
    "\n",
    "Question: How many of each item will you need in total?\n",
    "\n",
    "Answer:\n",
    "\"\"\"\n",
    "\n",
    "logger.info(f\"\\u001b[4mPrompt\\u001b[0m\\n{prompt}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 392,
   "metadata": {
    "tags": [],
    "ExecuteTime": {
     "end_time": "2023-07-10T09:43:48.771360Z",
     "start_time": "2023-07-10T09:43:46.183739Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[4mJurassic\u001B[0m: 80 cups x $3 = $240\n",
      "60 slices x $2 = $120\n",
      "20 slices x $4 = $80\n",
      "\n",
      "Total cost: $240 + $120 + $80 = $440\n"
     ]
    }
   ],
   "source": [
    "response = query_endpoint_with_json_payload(prompt)\n",
    "qty_response = response[\"jurassic\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 394,
   "metadata": {
    "tags": [],
    "ExecuteTime": {
     "end_time": "2023-07-10T09:43:51.910501Z",
     "start_time": "2023-07-10T09:43:51.871902Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[4mPrompt\u001B[0m\n",
      "You are planning a party that will need the following supplies:\n",
      "80 cups x $3 = $240\n",
      "60 slices x $2 = $120\n",
      "20 slices x $4 = $80\n",
      "\n",
      "Total cost: $240 + $120 + $80 = $440\n",
      "\n",
      "The costs of different items are:\n",
      "\n",
      "- Drinks are $3 per cup\n",
      "- Pizza slices are $2 each\n",
      "- Cake slices are $4 each\n",
      "\n",
      "Question: How much will the party cost in total?\n",
      "\n",
      "Answer:\n",
      "\n"
     ]
    }
   ],
   "source": [
    "prompt = f\"\"\"You are planning a party that will need the following supplies:\n",
    "{qty_response}\n",
    "\n",
    "The costs of different items are:\n",
    "\n",
    "- Drinks are $3 per cup\n",
    "- Pizza slices are $2 each\n",
    "- Cake slices are $4 each\n",
    "\n",
    "Question: How much will the party cost in total?\n",
    "\n",
    "Answer:\n",
    "\"\"\"\n",
    "\n",
    "logger.info(f\"\\u001b[4mPrompt\\u001b[0m\\n{prompt}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 390,
   "metadata": {
    "tags": [],
    "ExecuteTime": {
     "end_time": "2023-07-10T09:43:20.397352Z",
     "start_time": "2023-07-10T09:43:17.757674Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[4mJurassic\u001B[0m: 80 cups x $3 = $240\n",
      "60 slices x $2 = $120\n",
      "20 slices x $4 = $80\n",
      "\n",
      "Total cost: $240 + $120 + $80 = $440\n"
     ]
    },
    {
     "data": {
      "text/plain": "{'jurassic': '80 cups x $3 = $240\\n60 slices x $2 = $120\\n20 slices x $4 = $80\\n\\nTotal cost: $240 + $120 + $80 = $440'}"
     },
     "execution_count": 390,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query_endpoint_with_json_payload(prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Tree of Thoughts (ToT) Prompting\n",
    "\n",
    "In section 3 we saw that as well as explicitly structuring a problem, you could prompt the model by example to suggest it's own step-by-step solution process.\n",
    "\n",
    "**Tree of Thoughts** prompting ([Yao et al, 2023](https://arxiv.org/abs/2305.10601)) goes a step further to help models explore a bigger solution space.\n",
    "\n",
    "1. Give the model the current problem context and have it suggest possible next steps or decide if the problem is unsolvable.\n",
    "2. Choose the most promising next step, and repeat step (1)\n",
    "3. If the problem is solved, exit. If the problem seems unsolvable from the current state, back-track to one of the previous states and try a different next step.\n",
    "\n",
    "This method can significantly out-perform CoT and other approaches for problems where sequential decision-making is required. We won't demonstrate it here from scratch, because it's a bit more complex to set up - but you can find open source implementations online. For more information see this [guide](https://www.promptingguide.ai/techniques/tot).\n",
    "\n",
    "Tree-of-Thoughts is an example of a chained, automatic prompt generation technique - concepts we'll discuss further in the next [Advanced_Prompt_Engineering.ipynb](Advanced_Prompt_Engineering.ipynb) notebook"
   ]
  }
 ],
 "metadata": {
  "availableInstances": [
   {
    "_defaultOrder": 0,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.t3.medium",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 1,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.t3.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 2,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.t3.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 3,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.t3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 4,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 5,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 6,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 7,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 8,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 9,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 10,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 11,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 12,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5d.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 13,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5d.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 14,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5d.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 15,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5d.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 16,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5d.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 17,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5d.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 18,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5d.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 19,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 20,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": true,
    "memoryGiB": 0,
    "name": "ml.geospatial.interactive",
    "supportedImageNames": [
     "sagemaker-geospatial-v1-0"
    ],
    "vcpuNum": 0
   },
   {
    "_defaultOrder": 21,
    "_isFastLaunch": true,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.c5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 22,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.c5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 23,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.c5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 24,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.c5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 25,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 72,
    "name": "ml.c5.9xlarge",
    "vcpuNum": 36
   },
   {
    "_defaultOrder": 26,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 96,
    "name": "ml.c5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 27,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 144,
    "name": "ml.c5.18xlarge",
    "vcpuNum": 72
   },
   {
    "_defaultOrder": 28,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.c5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 29,
    "_isFastLaunch": true,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g4dn.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 30,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g4dn.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 31,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g4dn.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 32,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g4dn.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 33,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g4dn.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 34,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g4dn.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 35,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 61,
    "name": "ml.p3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 36,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 244,
    "name": "ml.p3.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 37,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 488,
    "name": "ml.p3.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 38,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.p3dn.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 39,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.r5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 40,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.r5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 41,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.r5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 42,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.r5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 43,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.r5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 44,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.r5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 45,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.r5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 46,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.r5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 47,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 48,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 49,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 50,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 51,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 52,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 53,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.g5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 54,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.g5.48xlarge",
    "vcpuNum": 192
   },
   {
    "_defaultOrder": 55,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 56,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4de.24xlarge",
    "vcpuNum": 96
   }
  ],
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
